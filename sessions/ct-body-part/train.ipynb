{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKYuZWZ0gP9z"
      },
      "source": [
        "# Deep Learning for Automatic Labeling of CT Images\n",
        "## By: Ian Pan, MD.ai modified by Anouk Stein, MD.ai to predict chest, abdomen, or pelvic slices. Note lower chest/upper abdomen may have labels for both chest and abdomen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyd2W4nWTOCv"
      },
      "source": [
        "!git clone https://github.com/rwfilice/bodypart.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtSWfduT3CoO"
      },
      "source": [
        "## Import Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dksRlUUOS-GM"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHp444ni3J7i"
      },
      "source": [
        "from scipy.ndimage.interpolation import zoom\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import glob\n",
        "import os \n",
        "import re \n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras import Model\n",
        "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras import optimizers\n",
        "\n",
        "import tensorflow as tf \n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(88) ; np.random.seed(88) \n",
        "\n",
        "# For data augmentation\n",
        "from albumentations import (\n",
        "    Compose, OneOf, HorizontalFlip, Blur, RandomGamma, RandomContrast, RandomBrightness\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEbNsZ4zSpLT"
      },
      "source": [
        "tf.compat.v1.enable_eager_execution()\n",
        "print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSK1Br4Gn9Ma"
      },
      "source": [
        "imagesPath = Path('bodypart/npy/')\n",
        "imageList = list(imagesPath.glob('**/*.npy'))\n",
        "testList = list(sorted(imagesPath.glob('**/5fd4ea78053ef3b10aace7cbf9d70b65*.npy'), key=lambda fn: int(re.search('-([0-9]*)', str(fn)).group(1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1_MvTfISpLW"
      },
      "source": [
        "testPath = Path('bodypart/testnpy')\n",
        "testList = list(sorted(testPath.glob('**/*.npy'), key=lambda fn: int(re.search('-([0-9]*)', str(fn)).group(1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPh6ihMaSpLY"
      },
      "source": [
        "testList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoaCWETfSpLb"
      },
      "source": [
        "df = pd.read_csv(\"bodypart/labels-overlap.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8C_G60bn-nf"
      },
      "source": [
        "## Locate DICOM images and split data into: training, validation, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC5ho-bmhQ5p"
      },
      "source": [
        "Let's locate all of the images we will use during training. In the previous code block, we kept track of images that the annotator excluded. We remove those images here. The data is structured as: exam (study) > series > images. That is, an exam can have multiple series and a series can have multiple images. The labels are assigned at the exam-level, so we can assume that all images in a series in an exam share the same labels. We split the data based on exams to prevent images from the same patient being distributed across the training, validation, and test data. We split the data into 80% training, 10% validation, 10% test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z3OFYEgQHe5"
      },
      "source": [
        "# Define a function to construct training/validation/test splits \n",
        "# Split data based on exams to prevent data leak \n",
        "# i.e. images from the same patient exist across the splits\n",
        "\n",
        "def get_train_val_test_split(images, train_frac, val_frac, seed=88):\n",
        "    '''\n",
        "    Test fraction will equal 1 - train_frac - val_frac.\n",
        "    This function splits data based on exams, extracts image file paths,\n",
        "    and removes images that cannot be read by pydicom.\n",
        "    '''\n",
        "    np.random.seed(seed) \n",
        "  \n",
        "    train_images = np.random.choice(images, int(train_frac*len(images)), replace=False)\n",
        "    not_train_images = list(set(images) - set(train_images)) \n",
        "    valid_images = np.random.choice(not_train_images, int(val_frac*len(images)), replace=False)\n",
        "    test_images = list(set(not_train_images) - set(valid_images)) \n",
        "    # Remove images that can't be read by pydicom\n",
        "    for im in train_images: \n",
        "        try: \n",
        "            _ = np.load(str(im)) \n",
        "        except:\n",
        "            train_images.remove(im) \n",
        "    for im in valid_images: \n",
        "        try: \n",
        "            _ = np.load(str(im))  \n",
        "        except:\n",
        "            valid_images.remove(im) \n",
        "    for im in test_images: \n",
        "        try: \n",
        "            _ = np.load(str(im))  \n",
        "        except:\n",
        "            test_images.remove(im) \n",
        "    return train_images, valid_images, test_images \n",
        "      \n",
        "# Let's do 3 random train/val/test splits 80%/10%/10%\n",
        "train0, val0, test0 = get_train_val_test_split(imageList, 0.8, 0.1, seed=0)\n",
        "train1, val1, test1 = get_train_val_test_split(imageList, 0.8, 0.1, seed=1)\n",
        "train2, val2, test2 = get_train_val_test_split(imageList, 0.8, 0.1, seed=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrYva5bISpLh"
      },
      "source": [
        "len(train0),len(val0),len(test0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgphczZ8UqYq"
      },
      "source": [
        "labels_dict = {'Chest': 0, \n",
        "               'Abdomen': 1,\n",
        "               'Pelvis': 2}\n",
        "N_CLASSES = len(labels_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APe2YcAX5rJp"
      },
      "source": [
        "## Set up data generation and augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbCXFROvigx6"
      },
      "source": [
        "Data generators are an efficient and effective way to load and augment data as it is being passed to the CNN. We convert the DICOM image array into an 8-bit image using a window width of 500 and level of 50. We had previously assigned an integer label to each label in our dataset (e.g., chest, abdomen, pelvis), but the CNN expects binary labels. Thus, for our 7 labels, we convert each integer into a length-7 vector, where each element in the vector is 1 if the image contains that label, 0 otherwise. \n",
        "\n",
        "We use simple data augmentation consisting of horizontal flips, random changes to brightness and contrast, and random levels of image blurring to help prevent the CNN from overfitting on the training data. The user should select data augmentations which represent the variability that could occur in a real setting. \n",
        "\n",
        "We also examine the class imbalance in our dataset by calculating the frequency of each label in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYc2ALDRUn3j"
      },
      "source": [
        "def get_dicom_and_uid(path_to_npy):\n",
        "    '''\n",
        "    Given a filepath, return the npy file and corresponding SOPInstanceUID. \n",
        "    '''\n",
        "    path_to_npy = str(path_to_npy)\n",
        "    dicom_file = np.load(path_to_npy)\n",
        "    uid = path_to_npy.split('/')[-1].replace('.npy', '')\n",
        "    return dicom_file, uid\n",
        "  \n",
        "def convert_dicom_to_8bit(npy_file, width, level, imsize=(224.,224.), clip=True): \n",
        "    '''\n",
        "    Given a DICOM file, window specifications, and image size, \n",
        "    return the image as a Numpy array scaled to [0,255] of the specified size. \n",
        "    '''\n",
        "    array = npy_file.copy() \n",
        "    #array = array + int(dicom_file.RescaleIntercept) #we did this on preprocess\n",
        "    #array = array * int(dicom_file.RescaleSlope) #we did this on preprocess\n",
        "    array = np.clip(array, level - width / 2, level + width / 2)\n",
        "    # Rescale to [0, 255]\n",
        "    array -= np.min(array) \n",
        "    array /= np.max(array) \n",
        "    array *= 255.\n",
        "    array = array.astype('uint8')\n",
        "    \n",
        "    if clip:\n",
        "    # Sometimes there is dead space around the images -- let's get rid of that\n",
        "        nonzeros = np.nonzero(array) \n",
        "        x1 = np.min(nonzeros[0]) ; x2 = np.max(nonzeros[0])\n",
        "        y1 = np.min(nonzeros[1]) ; y2 = np.max(nonzeros[1])\n",
        "        array = array[x1:x2,y1:y2]\n",
        "\n",
        "    # Resize image if necessary\n",
        "    resize_x = float(imsize[0]) / array.shape[0] \n",
        "    resize_y = float(imsize[1]) / array.shape[1] \n",
        "    if resize_x != 1. or resize_y != 1.:\n",
        "        array = zoom(array, [resize_x, resize_y], order=1, prefilter=False)\n",
        "    return np.expand_dims(array, axis=-1)\n",
        "\n",
        "def get_label_from_sop_id(df, uid):\n",
        "    '''\n",
        "    Given the annotations dataframe and a study ID, return a one-hot encoded\n",
        "    vector with labels for that study ID. \n",
        "    '''\n",
        "    df = df[df.npyid == uid] \n",
        "    labels = np.zeros((N_CLASSES,))\n",
        "    for rownum, row in df.iterrows():\n",
        "        lbls = row.labels.split(\"-\")\n",
        "        for lbl in lbls:\n",
        "            label_index = labels_dict[lbl] \n",
        "            labels[label_index] += 1 \n",
        "    return labels\n",
        "\n",
        "# Data augmentation involves perturbing the images in your training set \n",
        "# to prevent overfitting\n",
        "def augment(p=0.5):\n",
        "    return Compose([\n",
        "        HorizontalFlip(p=0.5),\n",
        "        Blur(p=0.5),\n",
        "        OneOf([\n",
        "            RandomGamma(),\n",
        "            RandomContrast(),\n",
        "            RandomBrightness(),\n",
        "        ], p=0.5)\n",
        "    ], p=p)\n",
        "\n",
        "aug = augment(p=0.5)\n",
        "\n",
        "def ScoutDataGenerator(df, images, imsize, batchsize, augment=True):\n",
        "    '''\n",
        "    Data generator to use with Keras when training. \n",
        "    '''\n",
        "    while True:\n",
        "        # Shuffle images\n",
        "        images = np.random.permutation(images) \n",
        "        for index in range(0, len(images), batchsize): \n",
        "            # Get images \n",
        "            image_batch = images[index:(index+batchsize)]\n",
        "            dicom_and_uids = [get_dicom_and_uid(im) for im in image_batch]\n",
        "            dicom_files = [_[0] for _ in dicom_and_uids]\n",
        "            uids = [_[1] for _ in dicom_and_uids] \n",
        "            array_list = [] ; uids_list = []\n",
        "            for ind, dcm in enumerate(dicom_files): \n",
        "                try:\n",
        "                    array_list.append(convert_dicom_to_8bit(dcm, WINDOW_WIDTH, \n",
        "                                                  WINDOW_LEVEL, \n",
        "                                                  imsize=(imsize,imsize)))\n",
        "                    uids_list.append(uids[ind])\n",
        "                except: \n",
        "                    continue\n",
        "        if augment: array_list = [aug(image=arr)['image'] for arr in array_list]\n",
        "        arrays = np.asarray(array_list)\n",
        "        # Data are labeled by studies\n",
        "        # All images in a study share the same label \n",
        "        arrays = preprocess_input(arrays, mode='tf')\n",
        "        labels = np.asarray([get_label_from_sop_id(df, _) for _ in uids_list])\n",
        "        yield arrays, labels\n",
        "      \n",
        "\n",
        "# Let's look at the distribution of labels in the training data\n",
        "dicom_and_uids = [get_dicom_and_uid(im) for im in train0] \n",
        "uids = [_[1] for _ in dicom_and_uids] \n",
        "labels = np.asarray([get_label_from_sop_id(df, _) for _ in uids]) \n",
        "class_frequencies = np.mean(labels, axis=0) \n",
        "\n",
        "category_list = ['Chest', 'Abdomen', 'Pelvis']\n",
        "\n",
        "for cat_index, cat in enumerate(category_list):\n",
        "    pct_frequency = round(class_frequencies[cat_index] * 100., 1)\n",
        "    print('Frequency of {} : {}%'.format(cat, pct_frequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqeeJOqN5xcP"
      },
      "source": [
        "## Set up Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z21oN8L5kLaE"
      },
      "source": [
        "Now we can set up a basic Keras CNN model. We will use the lightweight MobileNetV2 model since our classification problem is relatively simple. Important parameters that can affect model performance include: the initial learning rate (how aggressively the model makes changes to its weights), dropout probability (a method to prevent overfitting by randomly turning neurons in the CNN off), and batch size (the number of images at each iteration to adjust the CNN weights). We use the Adam optimizer, which is a popular optimizer that performs well in most cases. We use an image size of 256 x 256. Oftentimes we would see better performance with higher image sizes up to a point, after which increases in image size do not improve or even worsen performance. However, the tradeoff is that you require more GPU memory and training time. \n",
        "\n",
        "Implementing early stopping and a learning rate annealing schedule can help improve performance. A model can quickly overfit and perfectly predict the training data, especially if the model is large and the dataset is small. Early stopping uses validation performance to determine when to stop training: if a model is no longer making improvements on the validation dataset, then stop training. How long we wait and how much progress is considered improvement are both tunable parameters. Reducing the learning rate when validation performance stagnates can also improve model performance. While the initial learning rate should be relatively large to speed up convergence, training using smaller learning rates later on in the process can help the model make smaller adjustments to better fit the specific classification task. In this example, we choose to monitor the validation loss as track changes in the loss to determine when to reduce the learning rate and stop training.\n",
        "\n",
        "We initialize our model with ImageNet pretrained weights. Our dataset is relatively small so training from scratch (randomly initialized weights) will likely result in worse performance and unstable model training. Even with larger datasets, initializing with pretrained weights can speed up model convergence. Because ImageNet is composed of 3-channel RGB natural color images and our CT scout images are 1-channel grayscale images, we need to slightly modify the first layer of the ImageNet pretrained weights.\n",
        "\n",
        "Note that we use binary crossentropy as our loss function versus categorical crossentropy. We previously discussed how our problem is multi-label in that a single image can have multiple labels (e.g., chest AND abdomen). Categorical crossentropy is better suited for multi-class problems where an image has only one label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT-XqAcItusp"
      },
      "source": [
        "#######################\n",
        "# TRAINING PARAMETERS #\n",
        "#######################\n",
        "\n",
        "INITIAL_LR = 1e-4\n",
        "N_CLASSES  = 3\n",
        "BATCH_SIZE = 4\n",
        "DROPOUT    = 0.5\n",
        "IMSIZE     = 256\n",
        "# Max number of epochs to train for\n",
        "EPOCHS     = 50 \n",
        "# Pick a performance metric to determine whether the model is improving\n",
        "MONITOR    = 'val_loss' \n",
        "# Define a minimum improvement threshold\n",
        "# The model must improve validation performance by at least this amount \n",
        "# to be considered improving\n",
        "MIN_DELTA  = 0.001 \n",
        "# If the model is not improving, we should reduce the learning rate \n",
        "ANNEAL_BY  = 0.5 \n",
        "# A model may not improve after 1 epoch but could improve after the next epoch\n",
        "# without making changes to the learning rate. How many epochs should we wait?\n",
        "PATIENCE   = 2 \n",
        "# It can be a good idea to let the model \"settle in\" to a new learning rate \n",
        "# after decreasing it. How long should we wait?\n",
        "COOLDOWN   = 1\n",
        "# Stopping model training when the model isn't improving validation performance\n",
        "# can help prevent overfitting. \n",
        "STOP_AFTER = 5\n",
        "\n",
        "# Load pretrained MobileNetV2 ImageNet weights\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "imagenet_weights = base_model.get_weights()\n",
        "print(imagenet_weights[0].shape) \n",
        "# Sum over axis 2 to allow for grayscale (1-channel) input\n",
        "imagenet_weights[0] = np.expand_dims(np.sum(imagenet_weights[0], axis=2), axis=2)\n",
        "print(imagenet_weights[0].shape)\n",
        "base_model = MobileNetV2(weights=None, include_top=False, input_shape=(IMSIZE,IMSIZE,1)) \n",
        "base_model.set_weights(imagenet_weights)\n",
        "x = GlobalAveragePooling2D()(base_model.output) \n",
        "x = Dropout(DROPOUT)(x) \n",
        "prediction = Dense(N_CLASSES, activation='sigmoid')(x) \n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=prediction)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=INITIAL_LR), \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "WINDOW_LEVEL, WINDOW_WIDTH = 50, 500\n",
        "train_scoutgen = ScoutDataGenerator(df, train1, IMSIZE, BATCH_SIZE, augment=True)\n",
        "valid_scoutgen = ScoutDataGenerator(df, val1, IMSIZE, BATCH_SIZE, augment=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blw6djmlQ04w"
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"bodypart/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ysPBgxn1Hx"
      },
      "source": [
        "Let's take a look at an image produced by our data generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvQtWNQk_BEP"
      },
      "source": [
        "# Show example image\n",
        "test_image = next(train_scoutgen)[0][0]\n",
        "plt.imshow(test_image[..., 0], cmap='gray'); plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUqn1oJCn40y"
      },
      "source": [
        "## Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhLp4N9dn7yR"
      },
      "source": [
        "Once we have all of our training hyperparameters set up, training the model is simple in Keras. We validate on the validation set after every epoch. \n",
        "\n",
        "When we examined the distribution of class labels previously, we saw that there was an imbalance: abdomen and pelvis labels were both >30% whereas lower and upper extremity labels were <10%. Severe class imbalance can cause problems during training as the network will learn to simply predict the more prevalent class. Two common strategies are 1) over-/under-sampling the data so that the distributions of class labels are more similar and 2) using a weighted loss function that gives more weight to less common classes. Though our data is not severely imbalanced, adjusting for class imbalance can still result in small performance boosts. We will use an inverse-frequency weighted loss function during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep8iH4OSz7mi"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvJ6cyB3l-cM"
      },
      "source": [
        "# this is on just MGUH images starting from scratch\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=MONITOR, patience=STOP_AFTER, min_delta=MIN_DELTA,\n",
        "                  restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor=MONITOR, factor=ANNEAL_BY, patience=PATIENCE,\n",
        "                      min_delta=MIN_DELTA, mode='min', cooldown=COOLDOWN, \n",
        "                      verbose=1)\n",
        "]\n",
        "\n",
        "# Let's weight each class in the loss function by the inverse of its frequency\n",
        "weights = {} ; total_weight = 0.\n",
        "for freq_index, freq in enumerate(class_frequencies): \n",
        "    weights[freq_index] = 1. / freq\n",
        "    total_weight += weights[freq_index]\n",
        "\n",
        "# Scale so that sum of weights equals the number of classes\n",
        "for each_class in weights.keys(): \n",
        "    weights[each_class] = weights[each_class] / total_weight * N_CLASSES\n",
        "\n",
        "model.fit_generator(train_scoutgen, epochs=EPOCHS, \n",
        "                    steps_per_epoch=len(train1) / BATCH_SIZE, \n",
        "                    validation_data=valid_scoutgen, \n",
        "                    validation_steps=len(val1) / BATCH_SIZE,\n",
        "                    callbacks=callbacks,\n",
        "                    class_weight=weights) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rdAVXFhZIlb"
      },
      "source": [
        "title = \"mguh-multilabel.h5\"\n",
        "model.save_weights(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vffywqr-sCt1"
      },
      "source": [
        "\n",
        "\n",
        "That concludes the training notebook. We trained a basic CNN to label CT scout exams with their anatomical regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1id0Y9NGTxF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}